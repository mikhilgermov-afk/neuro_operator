# -*- coding: utf-8 -*-
import asyncio
import audioop
import numpy as np
import os
import sys
import struct
import time
import scipy.signal
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger(__name__).info
error_log = logging.getLogger(__name__).error

log("Starting Voice Bot service...")

try:
    log("Importing libraries...")
    from faster_whisper import WhisperModel
    from openai import OpenAI
    from tts_engine import F5TTSWrapper
    import prompts
    log("Libraries imported.")
except ImportError as e:
    error_log(f"CRITICAL ERROR importing libraries: {e}")
    sys.exit(1)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
RTP_IP = "0.0.0.0"
RTP_PORT = 10000
SAMPLE_RATE_TELEPHONY = 8000
SAMPLE_RATE_WHISPER = 16000
CHUNK_SIZE_20MS = 160 

stt_model = None
llm_client = None
tts_engine = None

def load_models():
    global stt_model, llm_client, tts_engine
    log("Loading models...")
    stt_model = WhisperModel("large-v3-turbo", device="cuda", compute_type="float16")
    llm_client = OpenAI(base_url=os.getenv("LLM_API_URL"), api_key="sk-local-key")
    tts_engine = F5TTSWrapper()
    log("All models loaded.")

class CallHandler:
    def __init__(self, transport, addr):
        self.transport = transport
        self.client_addr = addr
        self.history = []
        self.audio_buffer = bytearray()
        self.silence_frames = 0
        self.seq_num = 0
        self.timestamp = 0
        self.ssrc = 123456
        self.is_speaking = False
        self.greeting_sent = False

    async def send_greeting(self):
        if self.greeting_sent: return
        self.greeting_sent = True
        try:
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ
            sr, audio = tts_engine.generate("–î–∞, —è —Å–ª—É—à–∞—é.")
            await self.stream_audio_back(audio, sr)
        except Exception as e:
            error_log(f"Greeting error: {e}")

    def process_incoming_audio(self, data):
        if len(data) <= 12: return
        payload = data[12:]
        try:
            pcm_data = audioop.ulaw2lin(payload, 2)
        except Exception:
            return 

        rms = audioop.rms(pcm_data, 2)
        
        # –ü–æ—Ä–æ–≥ VAD (300)
        if rms > 300: 
            self.silence_frames = 0
            self.audio_buffer.extend(pcm_data)
        else:
            self.silence_frames += 1

        # –ñ–¥–µ–º ~1 —Å–µ–∫—É–Ω–¥—É —Ç–∏—à–∏–Ω—ã
        if self.silence_frames > 50 and len(self.audio_buffer) > 4000:
            audio_to_process = self.audio_buffer[:]
            self.audio_buffer = bytearray()
            self.silence_frames = 0
            asyncio.create_task(self.handle_turn(audio_to_process))

    async def handle_turn(self, audio_bytes):
        if not audio_bytes or self.is_speaking: return
        self.is_speaking = True
        
        try:
            # 1. –ë–∞–π—Ç—ã -> Float32
            audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            
            # 2. –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –¥–ª—è Whisper (8k -> 16k)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º resample_poly –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            if len(audio_np) < 100: return
            audio_16k = scipy.signal.resample_poly(audio_np, SAMPLE_RATE_WHISPER, SAMPLE_RATE_TELEPHONY)

            # 3. STT
            segments, _ = stt_model.transcribe(audio_16k, language="ru", beam_size=1)
            user_text = " ".join([s.text for s in segments]).strip()
            
            if len(user_text) < 2:
                self.is_speaking = False
                return

            log(f"üó£Ô∏è User: {user_text}")
            self.history.append({"role": "user", "content": user_text})

            # 4. LLM
            messages = prompts.create_messages(self.history)
            completion = await asyncio.to_thread(
                llm_client.chat.completions.create,
                model="Qwen/Qwen2.5-7B-Instruct", messages=messages, temperature=0.6, max_tokens=150
            )
            bot_text = completion.choices[0].message.content
            log(f"ü§ñ Bot: {bot_text}")
            self.history.append({"role": "assistant", "content": bot_text})

            # 5. TTS
            sr_out, audio_out = await asyncio.to_thread(tts_engine.generate, bot_text)
            
            # 6. –û—Ç–ø—Ä–∞–≤–∫–∞
            await self.stream_audio_back(audio_out, sr_out)

        except Exception as e:
            error_log(f"Error: {e}")
        finally:
            self.is_speaking = False

    async def stream_audio_back(self, audio_np, sr_in):
        if len(audio_np) == 0: return

        # --- 1. –£–º–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---
        # –ü–æ–¥–Ω–∏–º–∞–µ–º –≥—Ä–æ–º–∫–æ—Å—Ç—å, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–∏–≥–Ω–∞–ª –Ω–µ –ø—É—Å—Ç–æ–π —à—É–º
        max_val = np.max(np.abs(audio_np))
        if max_val > 0.05: # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –∫–∞–∫–æ–π-—Ç–æ –≥–æ–ª–æ—Å
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 90% (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –æ—Ç –∫–ª–∏–ø–ø–∏–Ω–≥–∞)
            audio_np = audio_np / max_val * 0.90
        
        # --- 2. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥ (24k -> 8k) ---
        # resample_poly —É–±–∏—Ä–∞–µ—Ç "—Ç—Ä—É–±–Ω—ã–π" –∑–≤–æ–Ω (aliasing)
        # up=1, down=3 (24000 * 1 / 3 = 8000)
        # –ï—Å–ª–∏ sr_in=24000, —Ç–æ up=1, down=3.
        # –í—ã—á–∏—Å–ª—è–µ–º –ù–û–î –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç:
        gcd = np.gcd(sr_in, SAMPLE_RATE_TELEPHONY)
        up = SAMPLE_RATE_TELEPHONY // gcd
        down = sr_in // gcd
        
        audio_8k = scipy.signal.resample_poly(audio_np, up, down)
        
        # --- 3. –ö–ª–∏–ø–ø–∏–Ω–≥ ---
        # –ñ–µ—Å—Ç–∫–æ —Å—Ä–µ–∑–∞–µ–º –ø–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        audio_8k = np.clip(audio_8k, -1.0, 1.0)
        
        # --- 4. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
        audio_int16 = (audio_8k * 32767).astype(np.int16)
        audio_bytes = audio_int16.tobytes()
        audio_ulaw = audioop.lin2ulaw(audio_bytes, 2)

        log(f"üîä Sending {len(audio_ulaw)} bytes...")
        
        chunk_size = CHUNK_SIZE_20MS
        for i in range(0, len(audio_ulaw), chunk_size):
            chunk = audio_ulaw[i : i + chunk_size]
            if len(chunk) < chunk_size: chunk += b'\xff' * (chunk_size - len(chunk))

            self.seq_num = (self.seq_num + 1) % 65536 
            self.timestamp = (self.timestamp + 160) % 4294967296
            header = struct.pack('!BBHII', 0x80, 0x00, self.seq_num, self.timestamp, self.ssrc)
            
            self.transport.sendto(header + chunk, self.client_addr)
            await asyncio.sleep(0.0195) 

class RTPProtocol(asyncio.DatagramProtocol):
    def __init__(self):
        self.calls = {}
        self.transport = None
    def connection_made(self, transport):
        self.transport = transport
        log(f"‚úÖ READY on {RTP_IP}:{RTP_PORT}")
    def datagram_received(self, data, addr):
        if addr not in self.calls:
            self.calls[addr] = CallHandler(self.transport, addr)
            asyncio.create_task(self.calls[addr].send_greeting())
        self.calls[addr].process_incoming_audio(data)

async def main():
    load_models()
    loop = asyncio.get_running_loop()
    transport, protocol = await loop.create_datagram_endpoint(lambda: RTPProtocol(), local_addr=(RTP_IP, RTP_PORT))
    try: await asyncio.Future()
    except asyncio.CancelledError: pass
    finally: transport.close()

if __name__ == "__main__":
    asyncio.run(main())
